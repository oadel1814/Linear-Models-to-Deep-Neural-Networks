{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Combined Linear and Neural Network Classification Notebook\n",
        "This notebook consolidates code for Binary Classification, Multiclass Softmax Regression (Manual/PyTorch), and Multilayer Neural Network analysis on the MNIST dataset."
      ],
      "metadata": {
        "id": "combined-header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Configuration"
      ],
      "metadata": {
        "id": "combined-setup-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import struct\n",
        "from array import array\n",
        "from os.path import join\n",
        "import random\n",
        "import seaborn as sns\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "combined-imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INPUT_SIZE = 28 * 28\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.01\n",
        "NUM_EPOCHS = 50\n",
        "SEED = 42\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "\n",
        "input_path = 'MNIST_Dataset'"
      ],
      "metadata": {
        "id": "combined-config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preparation Utilities (Multiclass and Binary)"
      ],
      "metadata": {
        "id": "combined-data-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistDataloader(object):\n",
        "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
        "                 test_images_filepath, test_labels_filepath):\n",
        "        self.training_images_filepath = training_images_filepath\n",
        "        self.training_labels_filepath = training_labels_filepath\n",
        "        self.test_images_filepath = test_images_filepath\n",
        "        self.test_labels_filepath = test_labels_filepath\n",
        "\n",
        "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
        "        labels = []\n",
        "        with open(labels_filepath, 'rb') as file:\n",
        "            magic, size = struct.unpack(\">II\", file.read(8))\n",
        "            if magic != 2049:\n",
        "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
        "            labels = array(\"B\", file.read())        \n",
        "        \n",
        "        with open(images_filepath, 'rb') as file:\n",
        "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
        "            if magic != 2051:\n",
        "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
        "            image_data = array(\"B\", file.read())        \n",
        "        images = []\n",
        "        for i in range(size):\n",
        "            images.append([0] * rows * cols)\n",
        "        for i in range(size):\n",
        "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
        "            img = img.reshape(28, 28)\n",
        "            images[i][:] = img            \n",
        "        \n",
        "        return images, labels\n",
        "            \n",
        "    def load_data(self):\n",
        "        training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
        "        training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
        "        test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
        "        test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
        "\n",
        "        x_train, y_train = self.read_images_labels(training_images_filepath, training_labels_filepath)\n",
        "        x_test, y_test = self.read_images_labels(test_images_filepath, test_labels_filepath)\n",
        "        return (x_train, y_train),(x_test, y_test)        "
      ],
      "metadata": {
        "id": "multiclass-data-loader"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_multiclass_data(random_state=SEED):\n",
        "    mnist_dataloader = MnistDataloader(\n",
        "        training_images_filepath=join(input_path, 'train-images.idx3-ubyte'),\n",
        "        training_labels_filepath=join(input_path, 'train-labels.idx1-ubyte'),\n",
        "        test_images_filepath=join(input_path, 't10k-images.idx3-ubyte'),\n",
        "        test_labels_filepath=join(input_path, 't10k-labels.idx1-ubyte')\n",
        "    )\n",
        "    (x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
        "\n",
        "    x_train = np.array(x_train, dtype=np.float32)\n",
        "    x_test = np.array(x_test, dtype=np.float32)\n",
        "    y_train = np.array(y_train, dtype=np.int64)\n",
        "    y_test = np.array(y_test, dtype=np.int64)\n",
        "\n",
        "    X = np.concatenate([x_train, x_test], axis=0)\n",
        "    y = np.concatenate([y_train, y_test], axis=0)\n",
        "\n",
        "    X_train_full, X_temp, y_train_full, y_temp = train_test_split(\n",
        "        X, y, test_size=0.4, stratify=y, random_state=random_state\n",
        "    )\n",
        "    X_val, X_test_final, y_val, y_test_final = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=random_state\n",
        "    )\n",
        "\n",
        "    X_train_flat = X_train_full.reshape(-1, INPUT_SIZE) / 255.0\n",
        "    X_val_flat = X_val.reshape(-1, INPUT_SIZE) / 255.0\n",
        "    X_test_flat = X_test_final.reshape(-1, INPUT_SIZE) / 255.0\n",
        "\n",
        "    X_train_t = torch.tensor(X_train_flat, dtype=torch.float32)\n",
        "    X_val_t = torch.tensor(X_val_flat, dtype=torch.float32)\n",
        "    X_test_t = torch.tensor(X_test_flat, dtype=torch.float32)\n",
        "    y_train_t = torch.tensor(y_train_full, dtype=torch.long)\n",
        "    y_val_t = torch.tensor(y_val, dtype=torch.long)\n",
        "    y_test_t = torch.tensor(y_test_final, dtype=torch.long)\n",
        "    \n",
        "    X_test_raw_np = X_test_final\n",
        "    y_test_raw_np = y_test_final\n",
        "\n",
        "    return X_train_t, X_val_t, X_test_t, y_train_t, y_val_t, y_test_t, X_train_flat, X_val_flat, X_test_flat, y_test_raw_np"
      ],
      "metadata": {
        "id": "multiclass-data-prep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryMNISTDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        self.data = data.float().reshape(-1, INPUT_SIZE)\n",
        "        self.targets = targets.float().unsqueeze(1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "def get_binary_dataloaders(batch_size=BATCH_SIZE, random_state=SEED):\n",
        "    train_data = datasets.MNIST(\n",
        "        root='./data', train=True, download=True\n",
        "    )\n",
        "\n",
        "    is_binary_mask = (train_data.targets == 0) | (train_data.targets == 1)\n",
        "    X_full = train_data.data[is_binary_mask]\n",
        "    Y_full = train_data.targets[is_binary_mask]\n",
        "\n",
        "    X_full = X_full.float() / 255.0\n",
        "\n",
        "    mean = 0.1307\n",
        "    std = 0.3081\n",
        "    X_full = (X_full - mean) / std\n",
        "\n",
        "\n",
        "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(\n",
        "        X_full, Y_full, test_size=0.2, random_state=random_state, stratify=Y_full\n",
        "    )\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
        "        X_train_val, Y_train_val, test_size=(0.2/0.8), random_state=random_state, stratify=Y_train_val\n",
        "    )\n",
        "\n",
        "    train_dataset = BinaryMNISTDataset(X_train, Y_train)\n",
        "    val_dataset = BinaryMNISTDataset(X_val, Y_val)\n",
        "    test_dataset = BinaryMNISTDataset(X_test, Y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "binary-data-prep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multiclass_dataloaders(batch_size=BATCH_SIZE):\n",
        "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "    val_dataset = TensorDataset(X_val_t, y_val_t)\n",
        "    test_dataset = TensorDataset(X_test_t, y_test_t)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "multiclass-dataloaders"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Binary Classification (Scratch Implementation)"
      ],
      "metadata": {
        "id": "binary-model-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryLogisticRegression:\n",
        "    def __init__(self, input_size):\n",
        "        self.W = torch.randn(input_size, 1, device=DEVICE) * 0.01\n",
        "        self.b = torch.zeros(1, device=DEVICE)\n",
        "        self.W.requires_grad_(True)\n",
        "        self.b.requires_grad_(True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        linear = torch.matmul(x, self.W) + self.b\n",
        "        y_pred = torch.sigmoid(linear)\n",
        "        return y_pred\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.W, self.b]\n",
        "\n",
        "def custom_binary_cross_entropy(y_pred, y_true):\n",
        "    epsilon = 1e-15\n",
        "    y_pred = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
        "    loss = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
        "    return loss.mean()\n",
        "\n",
        "def calculate_binary_accuracy(y_pred, y_true):\n",
        "    y_pred_class = (y_pred >= 0.5).float()\n",
        "    correct = (y_pred_class == y_true).float().sum()\n",
        "    return (correct / len(y_true)).item()\n",
        "\n",
        "def train_model_binary(model, optimizer, train_loader, val_loader, loss_fn, epochs):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    print(f\"Starting binary training on {DEVICE}...\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_train_loss, epoch_train_acc = 0.0, 0.0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "\n",
        "            y_pred = model.forward(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += loss.item() * len(X_batch)\n",
        "            epoch_train_acc += calculate_binary_accuracy(y_pred, y_batch) * len(X_batch)\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader.dataset)\n",
        "        avg_train_acc = epoch_train_acc / len(train_loader.dataset)\n",
        "\n",
        "        epoch_val_loss, epoch_val_acc = 0.0, 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch_val, y_batch_val in val_loader:\n",
        "                X_batch_val, y_batch_val = X_batch_val.to(DEVICE), y_batch_val.to(DEVICE)\n",
        "\n",
        "                y_pred_val = model.forward(X_batch_val)\n",
        "                loss_val = loss_fn(y_pred_val, y_batch_val)\n",
        "\n",
        "                epoch_val_loss += loss_val.item() * len(X_batch_val)\n",
        "                epoch_val_acc += calculate_binary_accuracy(y_pred_val, y_batch_val) * len(X_batch_val)\n",
        "\n",
        "        avg_val_loss = epoch_val_loss / len(val_loader.dataset)\n",
        "        avg_val_acc = epoch_val_acc / len(val_loader.dataset)\n",
        "\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accuracies.append(avg_train_acc)\n",
        "        val_accuracies.append(avg_val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1:2d}/{epochs} | Train Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.4f} | Val Loss: {avg_val_loss:.4f}, Acc: {avg_val_acc:.4f}')\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "def evaluate_binary_model(model, test_loader, loss_fn):\n",
        "    all_preds, all_targets = [], []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "\n",
        "            y_pred = model.forward(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "\n",
        "            total_loss += loss.item() * len(X_batch)\n",
        "            y_pred_class = (y_pred >= 0.5).float()\n",
        "\n",
        "            all_preds.extend(y_pred_class.cpu().numpy().flatten())\n",
        "            all_targets.extend(y_batch.cpu().numpy().flatten())\n",
        "\n",
        "    avg_loss = total_loss / len(test_loader.dataset)\n",
        "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
        "    final_acc = (conf_matrix[0, 0] + conf_matrix[1, 1]) / len(all_targets)\n",
        "\n",
        "    return final_acc, conf_matrix, avg_loss"
      ],
      "metadata": {
        "id": "binary-model-funcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Multiclass Softmax Regression (Scratch/NumPy Implementation)"
      ],
      "metadata": {
        "id": "softmax-manual-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(input_dim=INPUT_SIZE, num_classes=10):\n",
        "    W = np.random.randn(input_dim, num_classes) * 0.01\n",
        "    b = np.zeros((1, num_classes))\n",
        "    return W, b\n",
        "\n",
        "def softmax_forward(X, W, b):\n",
        "    logits = X @ W + b\n",
        "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
        "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
        "    return logits, probs\n",
        "\n",
        "def calc_loss_and_accuracy(X, y, W, b):\n",
        "    _, probs = softmax_forward(X, W, b)\n",
        "    predicted_classes = np.argmax(probs, axis=1)\n",
        "    accuracy = np.mean(predicted_classes == y) * 100\n",
        "    \n",
        "    n_samples = len(y)\n",
        "    correct_class_probs = probs[np.arange(n_samples), y]\n",
        "    loss = -np.mean(np.log(correct_class_probs + 1e-8))\n",
        "    \n",
        "    return loss, accuracy, predicted_classes\n",
        "\n",
        "def train_epoch_sgd(X, y, W, b, learning_rate, batch_size):\n",
        "    n_train = len(X)\n",
        "    n_batches = n_train // batch_size\n",
        "    \n",
        "    indices = np.random.permutation(n_train)\n",
        "    X_shuffled = X[indices]\n",
        "    y_shuffled = y[indices]\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i in range(n_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        X_batch = X_shuffled[start_idx:end_idx]\n",
        "        y_batch = y_shuffled[start_idx:end_idx]\n",
        "        \n",
        "        logits, probs = softmax_forward(X_batch, W, b)\n",
        "        \n",
        "        batch_size_actual = len(y_batch)\n",
        "        correct_probs = probs[np.arange(batch_size_actual), y_batch]\n",
        "        batch_loss = -np.mean(np.log(correct_probs + 1e-8))\n",
        "        epoch_loss += batch_loss\n",
        "        \n",
        "        y_one_hot = np.zeros_like(probs)\n",
        "        y_one_hot[np.arange(batch_size_actual), y_batch] = 1\n",
        "        dlogits = (probs - y_one_hot) / batch_size_actual\n",
        "        dW = X_batch.T @ dlogits\n",
        "        db = np.sum(dlogits, axis=0, keepdims=True)\n",
        "\n",
        "        W -= learning_rate * dW\n",
        "        b -= learning_rate * db\n",
        "    \n",
        "    return W, b, epoch_loss\n",
        "\n",
        "def train_manual_softmax(X_train, y_train, X_val, y_val, learning_rate=LEARNING_RATE, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE):\n",
        "    W, b = initialize_weights()\n",
        "    \n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    n_train = len(X_train)\n",
        "    n_batches = n_train // batch_size\n",
        "    \n",
        "    for i in range(num_epochs):\n",
        "        W, b, l = train_epoch_sgd(X_train, y_train, W, b, learning_rate, batch_size)\n",
        "        \n",
        "        train_loss, train_acc, _ = calc_loss_and_accuracy(X_train, y_train, W, b)\n",
        "        \n",
        "        val_loss, val_acc, _ = calc_loss_and_accuracy(X_val, y_val, W, b)\n",
        "        \n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_acc)\n",
        "    \n",
        "    return W, b, train_losses, train_accuracies, val_losses, val_accuracies"
      ],
      "metadata": {
        "id": "softmax-manual-funcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. General PyTorch Utilities and Model Definitions"
      ],
      "metadata": {
        "id": "combined-pytorch-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pytorch_model_comp(model, train_loader, val_loader, criterion, optimizer, num_epochs, verbose=False):\n",
        "    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss, correct, total = 0, 0, 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "\n",
        "        train_loss = epoch_loss / len(train_loader)\n",
        "        train_acc = 100 * correct / total\n",
        "\n",
        "        val_loss, val_acc, _ = evaluate_pytorch_model_comp(model, val_loader, criterion)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "    \n",
        "        if verbose and (epoch + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    return train_losses, train_accs, val_losses, val_accs, training_time\n",
        "\n",
        "def evaluate_pytorch_model_comp(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictions = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += (predicted == y_batch).sum().item()\n",
        "            predictions.extend(predicted.cpu().numpy())\n",
        "    \n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    \n",
        "    return avg_loss, accuracy, np.array(predictions)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "class SoftmaxRegressionPyTorch(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_SIZE, num_classes=10):\n",
        "        super(SoftmaxRegressionPyTorch, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=INPUT_SIZE, hidden_layers=[256, 128], num_classes=10):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        \n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "        \n",
        "        for hidden_dim in hidden_layers:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            prev_dim = hidden_dim\n",
        "        \n",
        "        layers.append(nn.Linear(prev_dim, num_classes))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "pytorch-comp-funcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Execution: Multiclass Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "multiclass-exec-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_t, X_val_t, X_test_t, y_train_t, y_val_t, y_test_t, X_train_flat, X_val_flat, X_test_flat, y_test_raw_np = load_and_prepare_multiclass_data()\n",
        "\n",
        "print(f\"Loaded Data Shapes:\")\n",
        "print(f\"  Train (tensors): {X_train_t.shape}, {y_train_t.shape}\")\n",
        "print(f\"  Validation (tensors): {X_val_t.shape}, {y_val_t.shape}\")\n",
        "print(f\"  Test (tensors): {X_test_t.shape}, {y_test_t.shape}\")\n",
        "\n",
        "X_train_np = X_train_flat\n",
        "X_val_np = X_val_flat\n",
        "X_test_np = X_test_flat\n",
        "y_train_np = y_train_t.numpy()\n",
        "y_val_np = y_val_t.numpy()\n",
        "y_test_np = y_test_raw_np"
      ],
      "metadata": {
        "id": "multiclass-data-exec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Execution: Binary Classification (0s and 1s)"
      ],
      "metadata": {
        "id": "binary-exec-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_binary, val_loader_binary, test_loader_binary = get_binary_dataloaders(num_epochs=20)\n",
        "\n",
        "model_binary = BinaryLogisticRegression(INPUT_SIZE)\n",
        "optimizer_binary = torch.optim.SGD(model_binary.parameters(), lr=0.01)\n",
        "\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = train_model_binary(\n",
        "    model=model_binary,\n",
        "    optimizer=optimizer_binary,\n",
        "    train_loader=train_loader_binary,\n",
        "    val_loader=val_loader_binary,\n",
        "    loss_fn=custom_binary_cross_entropy,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "test_acc, conf_matrix, test_loss = evaluate_binary_model(\n",
        "    model=model_binary,\n",
        "    test_loader=test_loader_binary,\n",
        "    loss_fn=custom_binary_cross_entropy\n",
        ")\n",
        "\n",
        "print(\"\\nFinal Test Set Results (Binary Classification)\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
      ],
      "metadata": {
        "id": "binary-exec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Execution: Multiclass Softmax Regression (Manual)"
      ],
      "metadata": {
        "id": "softmax-manual-exec-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W, b = initialize_weights()\n",
        "initial_loss, initial_accuracy,_= calc_loss_and_accuracy(X_train_np, y_train_np, W, b)\n",
        "\n",
        "print(f\"Initial accuracy with random weights: {initial_accuracy:.2f}%\")\n",
        "print(f\"Initial loss: {initial_loss:.4f}\")\n",
        "\n",
        "W_manual, b_manual, train_losses_manual, train_accuracies_manual, val_losses_manual, val_accuracies_manual = train_manual_softmax(X_train_np, y_train_np, X_val_np, y_val_np, learning_rate=0.01, num_epochs=NUM_EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_loss_manual, test_acc_manual, test_pred_manual = calc_loss_and_accuracy(X_test_np, y_test_np, W_manual, b_manual)\n",
        "\n",
        "print(\"\\nFinal Manual Softmax Test Results:\")\n",
        "print(f\"Test Loss: {test_loss_manual:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc_manual:.2f}%\")"
      ],
      "metadata": {
        "id": "softmax-manual-exec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Hyperparameter Analysis (Using Neural Network Model)"
      ],
      "metadata": {
        "id": "combined-hp-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_results(results_dict, title_prefix=''):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    colors = ['blue', 'green', 'orange', 'red', 'purple']\n",
        "    \n",
        "    for i, (name, data) in enumerate(results_dict.items()):\n",
        "        color = colors[i % len(colors)]\n",
        "        \n",
        "        ax1.plot(data['train_losses'], label=name, color=color, linewidth=2)\n",
        "        \n",
        "        ax2.plot(data['val_accs'], label=name, color=color, linewidth=2)\n",
        "    \n",
        "    ax1.set_xlabel('Epoch', fontsize=12)\n",
        "    ax1.set_ylabel('Training Loss', fontsize=12)\n",
        "    ax1.set_title(f'{title_prefix} Training Loss', fontsize=14, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    ax2.set_xlabel('Epoch', fontsize=12)\n",
        "    ax2.set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
        "    ax2.set_title(f'{title_prefix} Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hp-plot-func"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.1: Learning Rate Analysis"
      ],
      "metadata": {
        "id": "lr-analysis-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"LEARNING RATE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.1, 1.0]\n",
        "results_lr = {}\n",
        "train_loader, val_loader, test_loader = create_multiclass_dataloaders(batch_size=64)\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"\\nTraining with learning rate = {lr}\")\n",
        "    model = NeuralNetwork(hidden_layers=[256, 128])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    train_losses, train_accs, val_losses, val_accs, train_time = train_pytorch_model_comp(\n",
        "        model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, verbose=False\n",
        "    )\n",
        "    test_loss, test_acc, _ = evaluate_pytorch_model_comp(model, test_loader, criterion)\n",
        "\n",
        "    results_lr[lr] = {\n",
        "        'train_losses': train_losses, 'train_accs': train_accs,\n",
        "        'val_losses': val_losses, 'val_accs': val_accs,\n",
        "        'test_acc': test_acc, 'train_time': train_time\n",
        "    }\n",
        "    print(f\"  Final Val Acc: {val_accs[-1]:.2f}%, Test Acc: {test_acc:.2f}%, Time: {train_time:.1f}s\")\n",
        "\n",
        "plot_results(results_lr, title_prefix='Learning Rate Effect on')\n",
        "\n",
        "print(\"\\nLearning Rate Summary:\")\n",
        "print(f\"{'LR':<10} {'Final Val Acc':<15} {'Test Acc':<12} {'Time (s)':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for lr, data in results_lr.items():\n",
        "    print(f\"{lr:<10} {data['val_accs'][-1]:<15.2f}% {data['test_acc']:<12.2f}% {data['train_time']:<10.1f}\")"
      ],
      "metadata": {
        "id": "lr-analysis-exec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2: Batch Size Analysis"
      ],
      "metadata": {
        "id": "bs-analysis-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"BATCH SIZE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "results_bs = {}\n",
        "best_lr = 0.01\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    print(f\"\\nTraining with batch size = {bs}\")\n",
        "    train_loader, val_loader, test_loader = create_multiclass_dataloaders(batch_size=bs)\n",
        "    model = NeuralNetwork(hidden_layers=[256, 128])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=best_lr)\n",
        "\n",
        "    train_losses, train_accs, val_losses, val_accs, train_time = train_pytorch_model_comp(\n",
        "        model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, verbose=False\n",
        "    )\n",
        "    test_loss, test_acc, _ = evaluate_pytorch_model_comp(model, test_loader, criterion)\n",
        "\n",
        "    results_bs[bs] = {\n",
        "        'train_losses': train_losses, 'train_accs': train_accs,\n",
        "        'val_losses': val_losses, 'val_accs': val_accs,\n",
        "        'test_acc': test_acc, 'train_time': train_time\n",
        "    }\n",
        "    print(f\"  Final Val Acc: {val_accs[-1]:.2f}%, Test Acc: {test_acc:.2f}%, Time: {train_time:.1f}s\")\n",
        "\n",
        "plot_results(results_bs, title_prefix='Batch Size Effect on')\n",
        "\n",
        "print(\"\\nBatch Size Summary:\")\n",
        "print(f\"{'BS':<10} {'Final Val Acc':<15} {'Test Acc':<12} {'Time (s)':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for bs, data in results_bs.items():\n",
        "    print(f\"{bs:<10} {data['val_accs'][-1]:<15.2f}% {data['test_acc']:<12.2f}% {data['train_time']:<10.1f}\")"
      ],
      "metadata": {
        "id": "bs-analysis-exec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3: Network Depth Analysis"
      ],
      "metadata": {
        "id": "depth-analysis-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"NETWORK DEPTH ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "architectures = {\n",
        "    '2_layers': [256, 128],\n",
        "    '3_layers': [256, 128, 64],\n",
        "    '4_layers': [256, 128, 64, 32],\n",
        "    '5_layers': [256, 128, 64, 32, 16]\n",
        "}\n",
        "results_arch = {}\n",
        "train_loader, val_loader, test_loader = create_multiclass_dataloaders(batch_size=64)\n",
        "\n",
        "for name, layers in architectures.items():\n",
        "    print(f\"\\nTraining architecture: {name} - {layers}\")\n",
        "    model = NeuralNetwork(hidden_layers=layers)\n",
        "    n_params = count_parameters(model)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    train_losses, train_accs, val_losses, val_accs, train_time = train_pytorch_model_comp(\n",
        "        model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, verbose=False\n",
        "    )\n",
        "    test_loss, test_acc, _ = evaluate_pytorch_model_comp(model, test_loader, criterion)\n",
        "\n",
        "    results_arch[name] = {\n",
        "        'layers': layers, 'n_params': n_params,\n",
        "        'train_losses': train_losses, 'train_accs': train_accs,\n",
        "        'val_losses': val_losses, 'val_accs': val_accs,\n",
        "        'test_acc': test_acc, 'train_time': train_time\n",
        "    }\n",
        "    print(f\"  Parameters: {n_params:,}, Final Val Acc: {val_accs[-1]:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "plot_results(results_arch, title_prefix='Network Depth Effect on')\n",
        "\n",
        "print(\"\\nNetwork Depth Summary:\")\n",
        "print(f\"{'Depth':<10} {'Parameters':<12} {'Final Val Acc':<15} {'Test Acc':<12}\")\n",
        "print(\"-\" * 50)\n",
        "for name, data in results_arch.items():\n",
        "    print(f\"{name:<10} {data['n_params']:,<12} {data['val_accs'][-1]:<15.2f}% {data['test_acc']:.2f}%\")"
      ],
      "metadata": {
        "id": "depth-analysis-exec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.4: Network Width Analysis"
      ],
      "metadata": {
        "id": "width-analysis-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"NETWORK WIDTH ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "architectures_width = {\n",
        "    'small': [64, 32],\n",
        "    'medium': [128, 64],\n",
        "    'large': [256, 128],\n",
        "    'xlarge': [512, 256]\n",
        "}\n",
        "results_neurons = {}\n",
        "train_loader, val_loader, test_loader = create_multiclass_dataloaders(batch_size=64)\n",
        "\n",
        "for name, layers in architectures_width.items():\n",
        "    print(f\"\\nTraining architecture: {name} - {layers}\")\n",
        "    model = NeuralNetwork(hidden_layers=layers)\n",
        "    n_params = count_parameters(model)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    train_losses, train_accs, val_losses, val_accs, train_time = train_pytorch_model_comp(\n",
        "        model, train_loader, val_loader, criterion, optimizer, num_epochs=NUM_EPOCHS, verbose=False\n",
        "    )\n",
        "    test_loss, test_acc, _ = evaluate_pytorch_model_comp(model, test_loader, criterion)\n",
        "\n",
        "    results_neurons[name] = {\n",
        "        'config': layers, 'n_params': n_params,\n",
        "        'train_losses': train_losses, 'train_accs': train_accs,\n",
        "        'val_losses': val_losses, 'val_accs': val_accs,\n",
        "        'test_acc': test_acc, 'train_time': train_time\n",
        "    }\n",
        "    print(f\"  Parameters: {n_params:,}, Final Val Acc: {val_accs[-1]:.2f}%, Test Acc: {test_acc:.2f}%\")\n",
        "\n",
        "plot_results(results_neurons, title_prefix='Network Width Effect on')\n",
        "\n",
        "print(\"\\nNetwork Width Summary:\")\n",
        "print(f\"{'Width':<10} {'Parameters':<12} {'Final Val Acc':<15} {'Test Acc':<12}\")\n",
        "print(\"-\" * 50)\n",
        "for name, data in results_neurons.items():\n",
        "    print(f\"{str(data['config']):<10} {data['n_params']:,<12} {data['val_accs'][-1]:<15.2f}% {data['test_acc']:.2f}%\")"
      ],
      "metadata": {
        "id": "width-analysis-exec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Final Model Comparison"
      ],
      "metadata": {
        "id": "final-comparison-md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrices(y_true, y_pred_manual, y_pred_pytorch, y_pred_nn):\n",
        "    cm_manual = confusion_matrix(y_true, y_pred_manual)\n",
        "    cm_pytorch = confusion_matrix(y_true, y_pred_pytorch)\n",
        "    cm_nn = confusion_matrix(y_true, y_pred_nn)\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(24, 7))\n",
        "\n",
        "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_manual, display_labels=np.arange(10))\n",
        "    disp1.plot(cmap='Blues', ax=axs[0], values_format='d')\n",
        "    axs[0].set_title('Softmax Regression (Manual)')\n",
        "\n",
        "    disp2 = ConfusionMatrixDisplay(confusion_matrix=cm_pytorch, display_labels=np.arange(10))\n",
        "    disp2.plot(cmap='Greens', ax=axs[1], values_format='d')\n",
        "    axs[1].set_title('Softmax Regression (PyTorch)')\n",
        "\n",
        "    disp3 = ConfusionMatrixDisplay(confusion_matrix=cm_nn, display_labels=np.arange(10))\n",
        "    disp3.plot(cmap='Reds', ax=axs[2], values_format='d')\n",
        "    axs[2].set_title('Neural Network')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return cm_manual, cm_pytorch, cm_nn\n",
        "\n",
        "def calc_per_class_accuracy(y_true, y_pred, num_classes=10):\n",
        "    per_class_acc = []\n",
        "    \n",
        "    for class_idx in range(num_classes):\n",
        "        class_mask = (y_true == class_idx)\n",
        "        class_correct = np.sum((y_pred == class_idx) & class_mask)\n",
        "        class_total = np.sum(class_mask)\n",
        "        acc = (class_correct / class_total) * 100 if class_total > 0 else 0\n",
        "        per_class_acc.append(acc)\n",
        "    \n",
        "    return per_class_acc\n",
        "\n",
        "def plot_per_class_accuracy_comp(acc_softmax, acc_nn, y_true):\n",
        "    print(\"PER-CLASS ACCURACY ANALYSIS\\n\")\n",
        "    \n",
        "    print(\"Class | Softmax Acc | NN Acc | Test Samples\")\n",
        "    print(\"-\" * 55)\n",
        "    for i in range(10):\n",
        "        print(f\"  {i}   |   {acc_softmax[i]:5.2f}%  |   {acc_nn[i]:5.2f}%   |     {np.sum(y_true == i):4d}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    x = np.arange(10)\n",
        "    width = 0.35\n",
        "\n",
        "    bars1 = ax.bar(x - width/2, acc_softmax, width, label='Softmax', color='steelblue', edgecolor='black')\n",
        "    bars2 = ax.bar(x + width/2, acc_nn, width, label='Neural Network', color='seagreen', edgecolor='black')\n",
        "\n",
        "    ax.set_xlabel('Digit Class', fontsize=12)\n",
        "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    ax.set_title('Per-Class Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_ylim([0, 105])\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    for bars in [bars1, bars2]:\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return np.argmax(acc_softmax), np.argmin(acc_softmax), np.argmax(acc_nn), np.argmin(acc_nn)"
      ],
      "metadata": {
        "id": "final-plot-funcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"FINAL MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_loader, val_loader, test_loader = create_multiclass_dataloaders(batch_size=64)\n",
        "final_models = {}\n",
        "\n",
        "# 1. Softmax Regression (Simple)\n",
        "print(\"\\n1. Training Softmax Regression...\")\n",
        "model1 = SoftmaxRegressionPyTorch(input_dim=INPUT_SIZE, num_classes=10).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = optim.SGD(model1.parameters(), lr=0.01)\n",
        "train_losses1, train_accs1, val_losses1, val_accs1, time1 = train_pytorch_model_comp(model1, train_loader, val_loader, criterion, optimizer, num_epochs=50)\n",
        "test_loss1, test_acc1, preds1 = evaluate_pytorch_model_comp(model1, test_loader, criterion)\n",
        "final_models['Softmax Regression'] = { 'model': model1, 'n_params': count_parameters(model1), 'test_acc': test_acc1, 'predictions': preds1 }\n",
        "print(f\" Test Accuracy: {test_acc1:.2f}%, Parameters: {count_parameters(model1):,}\")\n",
        "\n",
        "# 2. Softmax with L2 Regularization\n",
        "print(\"\\n2. Training Softmax with L2 Regularization...\")\n",
        "model2 = SoftmaxRegressionPyTorch(input_dim=INPUT_SIZE, num_classes=10).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = optim.SGD(model2.parameters(), lr=0.01, weight_decay=0.001)\n",
        "train_losses2, train_accs2, val_losses2, val_accs2, time2 = train_pytorch_model_comp(model2, train_loader, val_loader, criterion, optimizer, num_epochs=50)\n",
        "test_loss2, test_acc2, preds2 = evaluate_pytorch_model_comp(model2, test_loader, criterion)\n",
        "final_models['Softmax + L2 Reg'] = { 'model': model2, 'n_params': count_parameters(model2), 'test_acc': test_acc2, 'predictions': preds2 }\n",
        "print(f\" Test Accuracy: {test_acc2:.2f}%, Parameters: {count_parameters(model2):,}\")\n",
        "\n",
        "# 3. Best Neural Network (3 layers, [256, 128, 64])\n",
        "print(\"\\n3. Training Best Neural Network...\")\n",
        "model3 = NeuralNetwork(input_dim=INPUT_SIZE, hidden_layers=[256, 128, 64], num_classes=10).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = optim.SGD(model3.parameters(), lr=0.01)\n",
        "train_losses3, train_accs3, val_losses3, val_accs3, time3 = train_pytorch_model_comp(model3, train_loader, val_loader, criterion, optimizer, num_epochs=50, verbose=True)\n",
        "test_loss3, test_acc3, preds3 = evaluate_pytorch_model_comp(model3, test_loader, criterion)\n",
        "final_models['Neural Network'] = { 'model': model3, 'n_params': count_parameters(model3), 'test_acc': test_acc3, 'predictions': preds3 }\n",
        "print(f\" Test Accuracy: {test_acc3:.2f}%, Parameters: {count_parameters(model3):,}\")\n",
        "\n",
        "cm_manual_softmax, cm_pytorch_softmax, cm_nn = plot_confusion_matrices(\n",
        "    y_test_np, \n",
        "    test_pred_manual, \n",
        "    final_models['Softmax Regression']['predictions'], \n",
        "    final_models['Neural Network']['predictions']\n",
        ")\n",
        "\n",
        "acc_softmax = calc_per_class_accuracy(y_test_np, final_models['Softmax Regression']['predictions'])\n",
        "acc_nn = calc_per_class_accuracy(y_test_np, final_models['Neural Network']['predictions'])\n",
        "plot_per_class_accuracy_comp(acc_softmax, acc_nn, y_test_np)"
      ],
      "metadata": {
        "id": "final-comparison-exec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7d7f3ed",
   "metadata": {},
   "source": [
    "# Binary Classification with PyTorch\n",
    "This notebook implements binary logistic regression for classifying MNIST digits (0s and 1s) using PyTorch. We'll build the model from scratch and implement the training loop manually to understand the underlying concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee9f82",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "First, we'll import the required libraries and set up our configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d9a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_SIZE = 28 * 28  # 784 pixels\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 20\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d84f2",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "We'll create a custom dataset class for handling binary MNIST data (0s and 1s only) and implement the data loading utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9557900",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset to filter MNIST for only digits 0 and 1, and flatten images.\n",
    "    Digit 1 is relabeled as 1 (Positive Class).\n",
    "    Digit 0 is relabeled as 0 (Negative Class).\n",
    "    \"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        # Flatten the 28x28 images into 784 features\n",
    "        self.data = data.float().reshape(-1, INPUT_SIZE)\n",
    "        # Reshape targets to (N, 1) and move to float for BCE loss\n",
    "        self.targets = targets.float().unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "def get_binary_dataloaders(batch_size=BATCH_SIZE):\n",
    "    \"\"\"Loads MNIST, filters for 0s and 1s, splits, and creates DataLoaders.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load full dataset\n",
    "    train_data = datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # Filter for digits 0 and 1\n",
    "    is_binary_mask = (train_data.targets == 0) | (train_data.targets == 1)\n",
    "    X_full = train_data.data[is_binary_mask]\n",
    "    Y_full = train_data.targets[is_binary_mask]\n",
    "\n",
    "    # Split into Train/Validation/Test (60% / 20% / 20%)\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(\n",
    "        X_full, Y_full, test_size=0.2, random_state=SEED, stratify=Y_full\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_train_val, Y_train_val, test_size=(0.2/0.8), random_state=SEED, stratify=Y_train_val\n",
    "    )\n",
    "\n",
    "    # Create Custom Datasets\n",
    "    train_dataset = BinaryMNISTDataset(X_train, Y_train)\n",
    "    val_dataset = BinaryMNISTDataset(X_val, Y_val)\n",
    "    test_dataset = BinaryMNISTDataset(X_test, Y_test)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494f575",
   "metadata": {},
   "source": [
    "## 3. Model, Loss, and Accuracy\n",
    "Here we implement the Binary Logistic Regression model and define our loss and accuracy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd6c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression Model implemented from scratch using pure PyTorch tensors.\n",
    "    W: (784, 1), b: (1,)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize Weights (W) with small random values and Bias (b) with zeros\n",
    "        self.W = torch.randn(input_size, 1, device=DEVICE) * 0.01\n",
    "        self.b = torch.zeros(1, device=DEVICE)\n",
    "        self.W.requires_grad_(True)\n",
    "        self.b.requires_grad_(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear Score (Logit): z = X @ W + b\n",
    "        linear = torch.matmul(x, self.W) + self.b\n",
    "        # Sigmoid Activation: y_hat = sigma(z)\n",
    "        y_pred = torch.sigmoid(linear)\n",
    "        return y_pred\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "\n",
    "def custom_binary_cross_entropy(y_pred, y_true):\n",
    "    # Clip predictions to prevent log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
    "    loss = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    return loss.mean()\n",
    "\n",
    "def calculate_binary_accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculates accuracy by converting probabilities to class labels (0 or 1).\"\"\"\n",
    "    y_pred_class = (y_pred >= 0.5).float()\n",
    "    correct = (y_pred_class == y_true).float().sum()\n",
    "    return (correct / len(y_true)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e03351",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Functions\n",
    "Here we implement the training loop and evaluation functions for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83260ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_binary(model, train_loader, val_loader, loss_fn, lr, epochs):\n",
    "    \"\"\"\n",
    "    The main training loop using manual Gradient Descent.\n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    print(f\"Starting training on {DEVICE}...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss, epoch_train_acc = 0.0, 0.0\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            # 1. Manual Gradient Zeroing\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.zero_()\n",
    "            \n",
    "            # 2. Forward Pass\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # 3. Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Gradient Descent Update\n",
    "            with torch.no_grad():\n",
    "                model.W -= lr * model.W.grad\n",
    "                model.b -= lr * model.b.grad\n",
    "                \n",
    "            # Accumulate metrics\n",
    "            epoch_train_loss += loss.item() * len(X_batch)\n",
    "            epoch_train_acc += calculate_binary_accuracy(y_pred, y_batch) * len(X_batch)\n",
    "            \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader.dataset)\n",
    "        avg_train_acc = epoch_train_acc / len(train_loader.dataset)\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        epoch_val_loss, epoch_val_acc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch_val, y_batch_val in val_loader:\n",
    "                X_batch_val, y_batch_val = X_batch_val.to(DEVICE), y_batch_val.to(DEVICE)\n",
    "                \n",
    "                y_pred_val = model.forward(X_batch_val)\n",
    "                loss_val = loss_fn(y_pred_val, y_batch_val)\n",
    "                \n",
    "                epoch_val_loss += loss_val.item() * len(X_batch_val)\n",
    "                epoch_val_acc += calculate_binary_accuracy(y_pred_val, y_batch_val) * len(X_batch_val)\n",
    "                \n",
    "        avg_val_loss = epoch_val_loss / len(val_loader.dataset)\n",
    "        avg_val_acc = epoch_val_acc / len(val_loader.dataset)\n",
    "        \n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(avg_train_acc)\n",
    "        val_accuracies.append(avg_val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d}/{epochs} | Train Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.4f} | Val Loss: {avg_val_loss:.4f}, Acc: {avg_val_acc:.4f}')\n",
    "        \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "def evaluate_binary_model(model, test_loader, loss_fn):\n",
    "    \"\"\"\n",
    "    Evaluates final model performance on the Test Set.\n",
    "    \"\"\"\n",
    "    all_preds, all_targets = [], []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            total_loss += loss.item() * len(X_batch)\n",
    "            y_pred_class = (y_pred >= 0.5).float()\n",
    "            \n",
    "            all_preds.extend(y_pred_class.cpu().numpy().flatten())\n",
    "            all_targets.extend(y_batch.cpu().numpy().flatten())\n",
    "            \n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    final_acc = (conf_matrix[0, 0] + conf_matrix[1, 1]) / len(all_targets)\n",
    "    \n",
    "    return final_acc, conf_matrix, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818723a",
   "metadata": {},
   "source": [
    "## 5. Plotting Utilities\n",
    "Function to visualize training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(train_metrics, val_metrics, title, ylabel):\n",
    "    \"\"\"Generates a plot for tracking convergence.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_metrics, label=f'Training {ylabel}')\n",
    "    plt.plot(val_metrics, label=f'Validation {ylabel}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ba0df",
   "metadata": {},
   "source": [
    "## 6. Model Training and Evaluation\n",
    "Let's train our model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and prepare binary data (0s and 1s only)\n",
    "train_loader_binary, val_loader_binary, test_loader_binary = get_binary_dataloaders()\n",
    "\n",
    "# 2. Instantiate the model\n",
    "model_binary = BinaryLogisticRegression(INPUT_SIZE)\n",
    "\n",
    "# 3. Train the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model_binary(\n",
    "    model=model_binary,\n",
    "    train_loader=train_loader_binary,\n",
    "    val_loader=val_loader_binary,\n",
    "    loss_fn=custom_binary_cross_entropy,\n",
    "    lr=LEARNING_RATE,\n",
    "    epochs=NUM_EPOCHS\n",
    ")\n",
    "\n",
    "# 4. Evaluate on the Test Set\n",
    "test_acc, conf_matrix, test_loss = evaluate_binary_model(\n",
    "    model=model_binary,\n",
    "    test_loader=test_loader_binary,\n",
    "    loss_fn=custom_binary_cross_entropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Output Results and Visualizations\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"FINAL TEST SET RESULTS (Binary Classification)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(\"\\nConfusion Matrix (Rows=True Class, Cols=Predicted Class):\")\n",
    "print(conf_matrix)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Plotting Loss and Accuracy Curves\n",
    "plot_metrics(train_losses, val_losses, \n",
    "            'Binary Logistic Regression Loss Over Epochs', 'Loss')\n",
    "plot_metrics(train_accuracies, val_accuracies, \n",
    "            'Binary Logistic Regression Accuracy Over Epochs', 'Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaf9c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

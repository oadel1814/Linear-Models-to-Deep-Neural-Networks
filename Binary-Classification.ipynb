{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd64f7cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# --- Configuration ---\n",
    "# Use CUDA if available, otherwise use CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "INPUT_SIZE = 28 * 28  # 784 pixels\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 20\n",
    "SEED = 42\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1. DATA PREPARATION UTILITIES (FILTERING 0s and 1s)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "class BinaryMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset to filter MNIST for only digits 0 and 1, and flatten images.\n",
    "    Digit 1 is relabeled as 1 (Positive Class).\n",
    "    Digit 0 is relabeled as 0 (Negative Class).\n",
    "    \"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        # Flatten the 28x28 images into 784 features\n",
    "        self.data = data.float().reshape(-1, INPUT_SIZE)\n",
    "        # Reshape targets to (N, 1) and move to float for BCE loss\n",
    "        self.targets = targets.float().unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "def get_binary_dataloaders(batch_size=BATCH_SIZE):\n",
    "    \"\"\"Loads MNIST, filters for 0s and 1s, splits, and creates DataLoaders.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize pixel values to [0, 1] range. Normalization is crucial for\n",
    "        # numerical stability in linear models.\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load full dataset\n",
    "    train_data = datasets.MNIST(\n",
    "        root='./data', train=True, download=True, transform=transform\n",
    "    )\n",
    "\n",
    "    # 1. Filter for digits 0 and 1\n",
    "    is_binary_mask = (train_data.targets == 0) | (train_data.targets == 1)\n",
    "    \n",
    "    X_full = train_data.data[is_binary_mask]\n",
    "    Y_full = train_data.targets[is_binary_mask]\n",
    "\n",
    "    # 2. Relabeling: Target 1 remains 1. Target 0 remains 0.\n",
    "    # The float() conversion will handle this: 1 -> 1.0, 0 -> 0.0\n",
    "    \n",
    "    # 3. Split into Train/Validation/Test (60% / 20% / 20% of the 0/1 data)\n",
    "    \n",
    "    # First split: 80% Train/Val, 20% Test\n",
    "    X_train_val, X_test, Y_train_val, Y_test = train_test_split(\n",
    "        X_full, Y_full, test_size=0.2, random_state=SEED, stratify=Y_full\n",
    "    )\n",
    "    \n",
    "    # Second split: 60% Train, 20% Validation (75% of X_train_val goes to train)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "        X_train_val, Y_train_val, test_size=(0.2/0.8), random_state=SEED, stratify=Y_train_val\n",
    "    )\n",
    "\n",
    "    # 4. Create Custom Datasets\n",
    "    train_dataset = BinaryMNISTDataset(X_train, Y_train)\n",
    "    val_dataset = BinaryMNISTDataset(X_val, Y_val)\n",
    "    test_dataset = BinaryMNISTDataset(X_test, Y_test)\n",
    "\n",
    "    # 5. Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1b382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 2. MODEL, LOSS, and ACCURACY\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "class BinaryLogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression Model implemented from scratch using pure PyTorch tensors.\n",
    "    W: (784, 1), b: (1,)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size):\n",
    "        # Initialize Weights (W) with small random values and Bias (b) with zeros.\n",
    "        # requires_grad=True is crucial for Autograd to track these tensors.\n",
    "        self.W = torch.randn(input_size, 1, device=DEVICE) * 0.01\n",
    "        self.b = torch.zeros(1, device=DEVICE)\n",
    "        self.W.requires_grad_(True)\n",
    "        self.b.requires_grad_(True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Linear Score (Logit): z = X @ W + b\n",
    "        # de 34an calculate el z score\n",
    "        linear = torch.matmul(x, self.W) + self.b\n",
    "        # 2. Sigmoid Activation: y_hat = sigma(z) (Probability P(y=1|X))\n",
    "        # => squashing the values bta3t el z [0,1] (probabilties)\n",
    "        y_pred = torch.sigmoid(linear)\n",
    "        return y_pred\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Returns the list of trainable parameters for the training loop\n",
    "        return [self.W, self.b]\n",
    "\n",
    "def custom_binary_cross_entropy(y_pred, y_true):\n",
    "    \n",
    "    # Clip predictions to prevent log(0) which causes instability (inf or NaN)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = torch.clamp(y_pred, epsilon, 1.0 - epsilon)\n",
    "    #l=-((tlog(y))+(1-t)log(1-y))\n",
    "    loss = - (y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "    return loss.mean()\n",
    "\n",
    "def calculate_binary_accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculates accuracy by converting probabilities to class labels (0 or 1).\"\"\"\n",
    "    # Decision Rule: If probability of P(y=1) >= 0.5, predict 1, else 0.\n",
    "    y_pred_class = (y_pred >= 0.5).float()  \n",
    "    correct = (y_pred_class == y_true).float().sum()  #count elsa7(true predictions) \n",
    "    #acc=(truepositives/total)\n",
    "    return (correct / len(y_true)).item()\n",
    "\n",
    "\n",
    "#this note to test and calc y(sigof z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "def train_model_binary(model, train_loader, val_loader, loss_fn, lr, epochs):\n",
    "    \"\"\"\n",
    "    The main training loop using manual Gradient Descent.\n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    print(f\"Starting training on {DEVICE}...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss, epoch_train_acc = 0.0, 0.0\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            # 1. Manual Gradient Zeroing (CRUCIAL)\n",
    "            # Clears accumulated gradients from the previous batch\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.zero_()\n",
    "            \n",
    "            # 2. Forward Pass: Get prediction and loss\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # 3. Backward Pass: Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # 4. Gradient Descent Optimization (Manual Update)\n",
    "            with torch.no_grad(): # Don't track this operation in the computation graph\n",
    "                model.W -= lr * model.W.grad # W_new = W_old - LR * dL/dW\n",
    "                model.b -= lr * model.b.grad # b_new = b_old - LR * dL/db\n",
    "                \n",
    "            # Accumulate metrics\n",
    "            epoch_train_loss += loss.item() * len(X_batch)\n",
    "            epoch_train_acc += calculate_binary_accuracy(y_pred, y_batch) * len(X_batch)\n",
    "            \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader.dataset)\n",
    "        avg_train_acc = epoch_train_acc / len(train_loader.dataset)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ec99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_binary(model, train_loader, val_loader, loss_fn, lr, epochs):\n",
    "    \"\"\"\n",
    "    The main training loop using manual Gradient Descent.\n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    print(f\"Starting training on {DEVICE}...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_train_loss, epoch_train_acc = 0.0, 0.0\n",
    "        \n",
    "        # --- TRAINING PHASE ---\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            # 1. Manual Gradient Zeroing (CRUCIAL)\n",
    "            # Clears accumulated gradients from the previous batch\n",
    "            for param in model.parameters():\n",
    "                if param.grad is not None:\n",
    "                    param.grad.zero_()\n",
    "            \n",
    "            # 2. Forward Pass: Get prediction and loss\n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # 3. Backward Pass: Compute gradients\n",
    "            loss.backward()\n",
    "            #function bt7sb el chain rule derivitaves\n",
    "            \n",
    "            # 4. Gradient Descent Optimization (Manual Update)\n",
    "            with torch.no_grad(): # Don't track this operation in the computation graph\n",
    "                model.W -= lr * model.W.grad # W_new = W_old - LR * dL/dW\n",
    "                model.b -= lr * model.b.grad # b_new = b_old - LR * dL/db\n",
    "                \n",
    "            # Accumulate metrics\n",
    "            epoch_train_loss += loss.item() * len(X_batch)\n",
    "            epoch_train_acc += calculate_binary_accuracy(y_pred, y_batch) * len(X_batch)\n",
    "            \n",
    "        avg_train_loss = epoch_train_loss / len(train_loader.dataset)\n",
    "        avg_train_acc = epoch_train_acc / len(train_loader.dataset)\n",
    "        \n",
    "        # --- VALIDATION PHASE ---\n",
    "        epoch_val_loss, epoch_val_acc = 0.0, 0.0\n",
    "        with torch.no_grad(): # Disable gradient tracking for evaluation\n",
    "            for X_batch_val, y_batch_val in val_loader:\n",
    "                X_batch_val, y_batch_val = X_batch_val.to(DEVICE), y_batch_val.to(DEVICE)\n",
    "                \n",
    "                y_pred_val = model.forward(X_batch_val)\n",
    "                loss_val = loss_fn(y_pred_val, y_batch_val)\n",
    "                \n",
    "                # Accumulate metrics\n",
    "                epoch_val_loss += loss_val.item() * len(X_batch_val)\n",
    "                epoch_val_acc += calculate_binary_accuracy(y_pred_val, y_batch_val) * len(X_batch_val)\n",
    "                \n",
    "        avg_val_loss = epoch_val_loss / len(val_loader.dataset)\n",
    "        avg_val_acc = epoch_val_acc / len(val_loader.dataset)\n",
    "        \n",
    "        # Store for plotting\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        train_accuracies.append(avg_train_acc)\n",
    "        val_accuracies.append(avg_val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1:2d}/{epochs} | Train Loss: {avg_train_loss:.4f}, Acc: {avg_train_acc:.4f} | Val Loss: {avg_val_loss:.4f}, Acc: {avg_val_acc:.4f}')\n",
    "        \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fc1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_model(model, test_loader, loss_fn):\n",
    "    \"\"\"\n",
    "    Evaluates final model performance on the Test Set.\n",
    "    \"\"\"\n",
    "    all_preds, all_targets = [], []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            y_pred = model.forward(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            total_loss += loss.item() * len(X_batch)\n",
    "            y_pred_class = (y_pred >= 0.5).float()\n",
    "            \n",
    "            # Move results to CPU for scikit-learn metrics\n",
    "            all_preds.extend(y_pred_class.cpu().numpy().flatten())\n",
    "            all_targets.extend(y_batch.cpu().numpy().flatten())\n",
    "            \n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    final_acc = (conf_matrix[0, 0] + conf_matrix[1, 1]) / len(all_targets)\n",
    "    \n",
    "    return final_acc, conf_matrix, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 4. PLOTTING FUNCTION\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def plot_metrics(train_metrics, val_metrics, title, ylabel):\n",
    "    \"\"\"Generates a plot for tracking convergence.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_metrics, label=f'Training {ylabel}')\n",
    "    plt.plot(val_metrics, label=f'Validation {ylabel}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# 5. EXECUTION BLOCK\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Load and prepare binary data (0s and 1s only)\n",
    "    train_loader_binary, val_loader_binary, test_loader_binary = get_binary_dataloaders()\n",
    "    \n",
    "    # 2. Instantiate the model\n",
    "    model_binary = BinaryLogisticRegression(INPUT_SIZE)\n",
    "\n",
    "    # 3. Train the model\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = train_model_binary(\n",
    "        model=model_binary,\n",
    "        train_loader=train_loader_binary,\n",
    "        val_loader=val_loader_binary,\n",
    "        loss_fn=custom_binary_cross_entropy,\n",
    "        lr=LEARNING_RATE,\n",
    "        epochs=NUM_EPOCHS\n",
    "    )\n",
    "\n",
    "    # 4. Evaluate on the Test Set\n",
    "    test_acc, conf_matrix, test_loss = evaluate_binary_model(\n",
    "        model=model_binary,\n",
    "        test_loader=test_loader_binary,\n",
    "        loss_fn=custom_binary_cross_entropy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8409d317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL TEST SET RESULTS (Binary Classification)\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINAL TEST SET RESULTS (Binary Classification)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtest_loss\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConfusion Matrix (Rows=True Class, Cols=Predicted Class):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    " # 5. Output Results and Visualizations\n",
    "    \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"FINAL TEST SET RESULTS (Binary Classification)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(\"\\nConfusion Matrix (Rows=True Class, Cols=Predicted Class):\")\n",
    "    # Rows: 0 (True Negatives), 1 (True Positives)\n",
    "    # Columns: 0 (Predicted Negatives), 1 (Predicted Positives)\n",
    "    # The matrix should be close to: [[TN, FP], [FN, TP]]\n",
    "print(conf_matrix)\n",
    "print(\"=\"*50)\n",
    "\n",
    "    # Plotting Loss and Accuracy Curves\n",
    "plot_metrics(train_losses, val_losses, \n",
    "                 'Binary Logistic Regression Loss Over Epochs', 'Loss')\n",
    "plot_metrics(train_accuracies, val_accuracies, \n",
    "                 'Binary Logistic Regression Accuracy Over Epochs', 'Accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
